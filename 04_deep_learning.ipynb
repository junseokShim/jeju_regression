{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#device = torch.device(\"mps\")\n",
    "\n",
    "# 데이터 불러오기 (시계열 가격 데이터)\n",
    "data = pd.read_csv('./processed_data/train_merge.csv')\n",
    "data = data[data['day_name']!='Sunday']\n",
    "PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        errors = torch.abs(predicted - target)\n",
    "        mae = torch.mean(errors)\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import math\n",
    "# dtype = torch.float\n",
    "# device = torch.device(\"mps\")\n",
    "\n",
    "# # Create random input and output data\n",
    "# x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "# y = torch.sin(x)\n",
    "\n",
    "# # Randomly initialize weights\n",
    "# a = torch.randn((), device=device, dtype=dtype)\n",
    "# b = torch.randn((), device=device, dtype=dtype)\n",
    "# c = torch.randn((), device=device, dtype=dtype)\n",
    "# d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "# learning_rate = 1e-6\n",
    "# for t in range(2000):\n",
    "#     # Forward pass: compute predicted y\n",
    "#     y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "#     # Compute and print loss\n",
    "#     loss = (y_pred - y).pow(2).sum().item()\n",
    "#     if t % 100 == 99:\n",
    "#         print(t, loss)\n",
    "\n",
    "# # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "#     grad_y_pred = 2.0 * (y_pred - y)\n",
    "#     grad_a = grad_y_pred.sum()\n",
    "#     grad_b = (grad_y_pred * x).sum()\n",
    "#     grad_c = (grad_y_pred * x ** 2).sum()\n",
    "#     grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "#     # Update weights using gradient descent\n",
    "#     a -= learning_rate * grad_a\n",
    "#     b -= learning_rate * grad_b\n",
    "#     c -= learning_rate * grad_c\n",
    "#     d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "# print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('./data/train.csv')\n",
    "\n",
    "# # %%\n",
    "# time = pd.to_datetime(data['timestamp'].copy())\n",
    "\n",
    "# data['Date'] = pd.to_datetime(data['timestamp'])\n",
    "# data['week'] = data['Date'].apply(lambda x: x.isocalendar()[1]) # 일요일 제거를 위함\n",
    "# data['day_name'] = data['Date'].dt.day_name()\n",
    "\n",
    "# data['year'] = data['timestamp'].apply(lambda x : int(x[0:4]))\n",
    "# data['month'] = data['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "# data['day'] = data['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "\n",
    "X = data.drop(columns=['ID', 'timestamp', 'supply(kg)_x', 'price(원/kg)', '기간', '품목명'])\n",
    "Y = data['price(원/kg)']\n",
    "\n",
    "#질적 변수들을 수치화합니다\n",
    "qual_col = ['item', 'corporation', 'location', 'day_name']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    X[i]=le.fit_transform(X[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['Date'])\n",
    "X = X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터에 Min-Max 스케일링 적용\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "#X# = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# 입력 텍스트와 레이블 생성\n",
    "Y = np.array(Y).reshape(Y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50934, 15)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 훈련 세트와 테스트 세트로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 정의 트랜스포머\n",
    "# class TimeSeriesTransformer(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads):\n",
    "#         super(TimeSeriesTransformer, self).__init__()\n",
    "        \n",
    "#         # Transformer 모델 불러오기\n",
    "#         transformer_config = BertConfig(\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_hidden_layers=num_layers,\n",
    "#             num_attention_heads=num_heads,\n",
    "#             intermediate_size=hidden_size * 4,\n",
    "#             hidden_dropout_prob=0.1,\n",
    "#             attention_probs_dropout_prob=0.1,\n",
    "#         )\n",
    "#         self.transformer = BertModel(transformer_config)\n",
    "        \n",
    "#         # Fully Connected Layer 추가\n",
    "#         self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "#         self.fc2 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "#         self.fc3 = nn.Linear(hidden_size//4, 1)\n",
    "    \n",
    "#     def forward(self, x, attention_mask=None):\n",
    "#         outputs = self.transformer(x, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs.last_hidden_state.mean(1)  # 각 시퀀스의 평균을 사용\n",
    "#         out = self.fc1(pooled_output)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.fc3(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.modules import Transformer\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "# 모델 정의 LSTM with Multi-Head Self-Attention\n",
    "class TimeSeriesLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nhead):\n",
    "        super(TimeSeriesLSTMWithAttention, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size*4, num_layers, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size*4, hidden_size*2, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*4, hidden_size*2)\n",
    "        self.fc2 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        # out, _ = self.lstm3(out)\n",
    "        #out, _ = self.lstm2(out)\n",
    "        \n",
    "        # Multi-Head Self-Attention 적용\n",
    "\n",
    "        #out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc2(out[:, -1, :])\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.modules import Transformer\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "# 모델 정의 LSTM with Multi-Head Self-Attention\n",
    "class TimeSeriesDNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nhead):\n",
    "        super(TimeSeriesDNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size*16)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size*16, hidden_size*16)\n",
    "        self.fc2 = nn.Linear(hidden_size*16, hidden_size*8)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size*8, hidden_size*4)\n",
    "        self.fc3 = nn.Linear(hidden_size*8, hidden_size*4)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size*4, hidden_size*2)\n",
    "        self.fc4 = nn.Linear(hidden_size*4, hidden_size*2)\n",
    "  \n",
    "        self.fc5 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "        self.fc6 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.fc6(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from torch.nn.modules import Transformer\n",
    "\n",
    "# # 모델 정의 Transformer with Multi-Head Self-Attention and Conv1d\n",
    "# # 모델 정의 Transformer with Multi-Head Self-Attention and Conv1d\n",
    "# class TimeSeriesTransformerWithAttentionAndConv1d(nn.Module):\n",
    "#     def __init__(self, d_model, nhead, num_filters, kernel_size):\n",
    "#         super(TimeSeriesTransformerWithAttentionAndConv1d, self).__init__()\n",
    "#         self.transformer = Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=3)  # Transformer 레이어 추가\n",
    "#         self.conv1d = nn.Conv1d(in_channels=d_model, out_channels=num_filters, kernel_size=kernel_size)\n",
    "\n",
    "#         self.fc1 = nn.Linear(num_filters, d_model)\n",
    "#         self.fc2 = nn.Linear(d_model, d_model // 2)\n",
    "#         self.fc3 = nn.Linear(d_model // 2, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 'src'와 'tgt'의 feature 차원을 동일하게 맞춤\n",
    "#         src = x\n",
    "#         tgt = x\n",
    "#         out = self.transformer(src, tgt)  # Transformer 레이어 적용\n",
    "\n",
    "#         # Conv1d 레이어를 통해 공간적 패턴을 감지\n",
    "#         out = out.permute(0, 2, 1)  # 컨볼루션을 위해 차원 변경\n",
    "#         out = self.conv1d(out)\n",
    "\n",
    "#         out = out.permute(0, 2, 1)  # 다시 차원 변경\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.fc3(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(TimeSeriesCNN, self).__init__()\n",
    "\n",
    "        # 1D CNN 레이어 정의\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=hidden_size*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size*2)\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=hidden_size*2, out_channels=hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layer 추가\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.fc3 = nn.Linear(hidden_size // 4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 텐서 차원을 (배치 크기, 시간 단계, 피쳐)로 변경\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.cnn3(out)\n",
    "\n",
    "        # CNN 이후 다시 차원을 변경하여 Fully Connected Layer에 적용\n",
    "        out = out.permute(0, 2, 1)\n",
    "\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     42\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 43\u001b[0m outputs \u001b[39m=\u001b[39m model(X_train_tensor)\n\u001b[1;32m     44\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, y_train_tensor)\n\u001b[1;32m     45\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[112], line 34\u001b[0m, in \u001b[0;36mTimeSeriesCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     33\u001b[0m     \u001b[39m# 입력 텐서 차원을 (배치 크기, 시간 단계, 피쳐)로 변경\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     36\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn1(x)\n\u001b[1;32m     37\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn2(out)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "#model = TimeSeriesLSTMWithAttention(input_size, hidden_size, num_layers, 2)\n",
    "#model = TimeSeriesDNN(input_size, hidden_size, num_layers, 2)\n",
    "model = TimeSeriesCNN(input_size, hidden_size, num_layers)\n",
    "\n",
    "# # 모델 파라미터 정의\n",
    "# input_size = X_train.shape[2]  # 입력 데이터의 특성 수\n",
    "# d_model = 64  # Transformer 레이어의 출력 차원\n",
    "# nhead = 4  # Multi-Head Self-Attention의 헤드 수\n",
    "# num_filters = 32  # Conv1d의 출력 채널 수\n",
    "# kernel_size = 3  # Conv1d의 커널 크기\n",
    "# model = TimeSeriesTransformerWithAttentionAndConv1d(d_model=64, nhead=4, num_filters=32, kernel_size=3)\n",
    "\n",
    "# 모델 훈련 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "#criterion = torch.nn.MSELoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Validation 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Early stopping 관련 설정\n",
    "best_val_loss = float('inf')\n",
    "patience = 30  # 일정 횟수 동안 검증 손실이 향상되지 않을 때 조기 종료\n",
    "counter = 0\n",
    "epoch = 0\n",
    "\n",
    "while 1:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation 손실 확인\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    epoch += 1\n",
    "    print(f\"epoch : {epoch} / Train loss : {math.sqrt(criterion(outputs, y_train_tensor))} / Validation loss: {math.sqrt(criterion(val_outputs, y_val_tensor))}\")\n",
    "        # 검증 손실이 이전 최고 손실보다 낮으면 모델 가중치 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './weights/best_model.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    # 검증 손실이 일정 횟수 동안 향상되지 않으면 조기 종료\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 15, 3], expected input[10187, 1, 15] to have 15 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m y_test_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y_test, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     test_outputs \u001b[39m=\u001b[39m model(X_test_tensor)\n\u001b[1;32m      7\u001b[0m     mse \u001b[39m=\u001b[39m mean_squared_error(y_test_tensor, test_outputs\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m평균 제곱 오차 (RMSE):\u001b[39m\u001b[39m\"\u001b[39m, math\u001b[39m.\u001b[39msqrt(mse))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[86], line 34\u001b[0m, in \u001b[0;36mTimeSeriesCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     32\u001b[0m     out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# CNN을 위해 차원 변경\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn1(out)\n\u001b[1;32m     35\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn2(out)\n\u001b[1;32m     36\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn3(out)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 15, 3], expected input[10187, 1, 15] to have 15 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터로 평가\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    mse = mean_squared_error(y_test_tensor, test_outputs.numpy())\n",
    "\n",
    "print(\"평균 제곱 오차 (RMSE):\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Transformer\n",
    "# # 모델 생성 및 훈련\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "# num_heads = 8\n",
    "# model = TimeSeriesTransformer(input_size, hidden_size, num_layers, num_heads)\n",
    "\n",
    "# # 모델 훈련 설정\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "# # Validation 데이터 분할\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# # Early stopping 관련 설정\n",
    "# best_val_loss = float('inf')\n",
    "# patience = 10  # 일정 횟수 동안 검증 손실이 향상되지 않을 때 조기 종료\n",
    "# counter = 0\n",
    "\n",
    "# for epoch in range(1000):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train_tensor)\n",
    "#     loss = criterion(outputs, y_train_tensor)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Validation 손실 확인\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_outputs = model(X_val_tensor)\n",
    "#         val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    \n",
    "#     print(f\"epoch : {epoch} / Train loss : {math.sqrt(loss)} / Validation loss: {math.sqrt(val_loss)}\")\n",
    "\n",
    "#         # 검증 손실이 이전 최고 손실보다 낮으면 모델 가중치 저장\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), './weights/best_model.pth')\n",
    "#         counter = 0\n",
    "#     else:\n",
    "#         counter += 1\n",
    "    \n",
    "#     # 검증 손실이 일정 횟수 동안 향상되지 않으면 조기 종료\n",
    "#     if counter >= patience:\n",
    "#         print(\"Early stopping.\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
