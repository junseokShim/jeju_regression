{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#device = torch.device(\"mps\")\n",
    "\n",
    "# 데이터 불러오기 (시계열 가격 데이터)\n",
    "data = pd.read_csv('./processed_data/train_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TG', 'CR', 'CB', 'RD', 'BC'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_lst = data['item'].unique()\n",
    "\n",
    "# for item in item_lst:\n",
    "#     data[data['item']==item].to_csv(f'./processed_data/{item}_train_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('./data/train.csv')\n",
    "\n",
    "# # %%\n",
    "# time = pd.to_datetime(data['timestamp'].copy())\n",
    "\n",
    "# data['Date'] = pd.to_datetime(data['timestamp'])\n",
    "# data['week'] = data['Date'].apply(lambda x: x.isocalendar()[1]) # 일요일 제거를 위함\n",
    "# data['day_name'] = data['Date'].dt.day_name()\n",
    "\n",
    "# data['year'] = data['timestamp'].apply(lambda x : int(x[0:4]))\n",
    "# data['month'] = data['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "# data['day'] = data['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "item = item_lst[0]\n",
    "data = pd.read_csv(f'./processed_data/{item}_train_merge.csv')\n",
    "\n",
    "X = data.drop(columns=['ID', 'timestamp', 'supply(kg)_x', 'price(원/kg)', '기간', '품목명'])\n",
    "Y = data['price(원/kg)']\n",
    "\n",
    "#질적 변수들을 수치화합니다\n",
    "qual_col = ['item', 'corporation', 'location', 'day_name']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    X[i]=le.fit_transform(X[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['Date'])\n",
    "X = X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터에 Min-Max 스케일링 적용\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "#X# = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# 입력 텍스트와 레이블 생성\n",
    "Y = np.array(Y).reshape(Y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15230, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 훈련 세트와 테스트 세트로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 정의 트랜스포머\n",
    "# class TimeSeriesTransformer(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads):\n",
    "#         super(TimeSeriesTransformer, self).__init__()\n",
    "        \n",
    "#         # Transformer 모델 불러오기\n",
    "#         transformer_config = BertConfig(\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_hidden_layers=num_layers,\n",
    "#             num_attention_heads=num_heads,\n",
    "#             intermediate_size=hidden_size * 4,\n",
    "#             hidden_dropout_prob=0.1,\n",
    "#             attention_probs_dropout_prob=0.1,\n",
    "#         )\n",
    "#         self.transformer = BertModel(transformer_config)\n",
    "        \n",
    "#         # Fully Connected Layer 추가\n",
    "#         self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "#         self.fc2 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "#         self.fc3 = nn.Linear(hidden_size//4, 1)\n",
    "    \n",
    "#     def forward(self, x, attention_mask=None):\n",
    "#         outputs = self.transformer(x, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs.last_hidden_state.mean(1)  # 각 시퀀스의 평균을 사용\n",
    "#         out = self.fc1(pooled_output)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.fc3(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.modules import Transformer\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "# 모델 정의 LSTM with Multi-Head Self-Attention\n",
    "class TimeSeriesLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nhead):\n",
    "        super(TimeSeriesLSTMWithAttention, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size*4, num_layers, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size*4, hidden_size*2, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*4, hidden_size*2)\n",
    "        self.fc2 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        # out, _ = self.lstm3(out)\n",
    "        #out, _ = self.lstm2(out)\n",
    "        \n",
    "        # Multi-Head Self-Attention 적용\n",
    "\n",
    "        #out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc2(out[:, -1, :])\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.modules import Transformer\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "# 모델 정의 LSTM with Multi-Head Self-Attention\n",
    "class TimeSeriesDNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nhead):\n",
    "        super(TimeSeriesDNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size*16)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size*16, hidden_size*16)\n",
    "        self.fc3 = nn.Linear(hidden_size*16, hidden_size*8)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size*8, hidden_size*8)\n",
    "        self.fc5 = nn.Linear(hidden_size*8, hidden_size*4)\n",
    "\n",
    "        self.fc6 = nn.Linear(hidden_size*4, hidden_size*4)\n",
    "        self.fc7 = nn.Linear(hidden_size*4, hidden_size*2)\n",
    "  \n",
    "        self.fc8 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.fc9 = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "        self.fc10 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.fc9(out)\n",
    "        out = self.fc10(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from torch.nn.modules import Transformer\n",
    "\n",
    "# # 모델 정의 Transformer with Multi-Head Self-Attention and Conv1d\n",
    "# # 모델 정의 Transformer with Multi-Head Self-Attention and Conv1d\n",
    "# class TimeSeriesTransformerWithAttentionAndConv1d(nn.Module):\n",
    "#     def __init__(self, d_model, nhead, num_filters, kernel_size):\n",
    "#         super(TimeSeriesTransformerWithAttentionAndConv1d, self).__init__()\n",
    "#         self.transformer = Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=3)  # Transformer 레이어 추가\n",
    "#         self.conv1d = nn.Conv1d(in_channels=d_model, out_channels=num_filters, kernel_size=kernel_size)\n",
    "\n",
    "#         self.fc1 = nn.Linear(num_filters, d_model)\n",
    "#         self.fc2 = nn.Linear(d_model, d_model // 2)\n",
    "#         self.fc3 = nn.Linear(d_model // 2, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 'src'와 'tgt'의 feature 차원을 동일하게 맞춤\n",
    "#         src = x\n",
    "#         tgt = x\n",
    "#         out = self.transformer(src, tgt)  # Transformer 레이어 적용\n",
    "\n",
    "#         # Conv1d 레이어를 통해 공간적 패턴을 감지\n",
    "#         out = out.permute(0, 2, 1)  # 컨볼루션을 위해 차원 변경\n",
    "#         out = self.conv1d(out)\n",
    "\n",
    "#         out = out.permute(0, 2, 1)  # 다시 차원 변경\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.fc3(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(TimeSeriesCNN, self).__init__()\n",
    "\n",
    "        # 1D CNN 레이어 정의\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=hidden_size*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size*2)\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=hidden_size*2, out_channels=hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layer 추가\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.fc3 = nn.Linear(hidden_size // 4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 텐서 차원을 (배치 크기, 시간 단계, 피쳐)로 변경\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.cnn3(out)\n",
    "\n",
    "        # CNN 이후 다시 차원을 변경하여 Fully Connected Layer에 적용\n",
    "        out = out.permute(0, 2, 1)\n",
    "\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 / Train loss : 4239.566723145184 / Validation loss: 4273.771636388636\n",
      "epoch : 2 / Train loss : 4239.502093406724 / Validation loss: 4273.645750410298\n",
      "epoch : 3 / Train loss : 4239.375897464154 / Validation loss: 4273.340847627299\n",
      "epoch : 4 / Train loss : 4239.070416966437 / Validation loss: 4272.629401200155\n",
      "epoch : 5 / Train loss : 4238.35770080818 / Validation loss: 4271.081596036302\n",
      "epoch : 6 / Train loss : 4236.807524540146 / Validation loss: 4267.915416219023\n",
      "epoch : 7 / Train loss : 4233.636262127393 / Validation loss: 4261.755976120641\n",
      "epoch : 8 / Train loss : 4227.467563447412 / Validation loss: 4250.26375652147\n",
      "epoch : 9 / Train loss : 4215.9584912567625 / Validation loss: 4229.53117969356\n",
      "epoch : 10 / Train loss : 4195.198922578046 / Validation loss: 4193.26507628602\n",
      "epoch : 11 / Train loss : 4158.8907174870565 / Validation loss: 4131.625346035141\n",
      "epoch : 12 / Train loss : 4097.194650001388 / Validation loss: 4029.8872192655713\n",
      "epoch : 13 / Train loss : 3995.3994794012774 / Validation loss: 3867.580897667171\n",
      "epoch : 14 / Train loss : 3833.108399197706 / Validation loss: 3621.2776750754697\n",
      "epoch : 15 / Train loss : 3587.1672667998073 / Validation loss: 3286.3604488856668\n",
      "epoch : 16 / Train loss : 3254.0301166399795 / Validation loss: 2987.438367565095\n",
      "epoch : 17 / Train loss : 2962.503839659959 / Validation loss: 3265.1203653158027\n",
      "epoch : 18 / Train loss : 3259.8323576527673 / Validation loss: 3320.734256154804\n",
      "epoch : 19 / Train loss : 3318.6580119078253 / Validation loss: 3072.053873225533\n",
      "epoch : 20 / Train loss : 3063.2420733595313 / Validation loss: 2934.46468712779\n",
      "epoch : 21 / Train loss : 2916.524644161266 / Validation loss: 2964.6463532772336\n",
      "epoch : 22 / Train loss : 2940.267164731804 / Validation loss: 3045.2870143879704\n",
      "epoch : 23 / Train loss : 3017.660849068364 / Validation loss: 3101.250231761378\n",
      "epoch : 24 / Train loss : 3072.3318505656252 / Validation loss: 3111.321744853785\n",
      "epoch : 25 / Train loss : 3082.298817441294 / Validation loss: 3075.4511864115157\n",
      "epoch : 26 / Train loss : 3047.332932254039 / Validation loss: 3002.3236001470596\n",
      "epoch : 27 / Train loss : 2976.3764882823543 / Validation loss: 2911.499441868399\n",
      "epoch : 28 / Train loss : 2889.6325025857527 / Validation loss: 2839.7270995643225\n",
      "epoch : 29 / Train loss : 2824.6215675732565 / Validation loss: 2830.2340009264253\n",
      "epoch : 30 / Train loss : 2824.326114314705 / Validation loss: 2876.8604415230157\n",
      "epoch : 31 / Train loss : 2879.6850522235936 / Validation loss: 2894.998272883768\n",
      "epoch : 32 / Train loss : 2901.897138080535 / Validation loss: 2842.335483365748\n",
      "epoch : 33 / Train loss : 2847.2402427614006 / Validation loss: 2778.5210994340136\n",
      "epoch : 34 / Train loss : 2777.421826082599 / Validation loss: 2758.1435785687445\n",
      "epoch : 35 / Train loss : 2750.397516723719 / Validation loss: 2774.3950691997707\n",
      "epoch : 36 / Train loss : 2761.6822771636857 / Validation loss: 2794.9538457727704\n",
      "epoch : 37 / Train loss : 2779.5807237783183 / Validation loss: 2797.265986637667\n",
      "epoch : 38 / Train loss : 2781.399018479729 / Validation loss: 2775.2044969695476\n",
      "epoch : 39 / Train loss : 2760.8888061636962 / Validation loss: 2735.9303719210398\n",
      "epoch : 40 / Train loss : 2725.2910853705152 / Validation loss: 2696.4432128268527\n",
      "epoch : 41 / Train loss : 2691.6450360327976 / Validation loss: 2675.6949564552383\n",
      "epoch : 42 / Train loss : 2678.2793916990813 / Validation loss: 2677.540569253807\n",
      "epoch : 43 / Train loss : 2687.1831906291764 / Validation loss: 2681.3152556161685\n",
      "epoch : 44 / Train loss : 2695.1372877833146 / Validation loss: 2667.026621539425\n",
      "epoch : 45 / Train loss : 2680.68088365624 / Validation loss: 2643.4503021619303\n",
      "epoch : 46 / Train loss : 2653.2589394930906 / Validation loss: 2631.2584441669733\n",
      "epoch : 47 / Train loss : 2635.6893785118154 / Validation loss: 2634.77854856912\n",
      "epoch : 48 / Train loss : 2634.4460518294923 / Validation loss: 2642.508088918556\n",
      "epoch : 49 / Train loss : 2639.2367267829536 / Validation loss: 2642.341385968134\n",
      "epoch : 50 / Train loss : 2638.329206145435 / Validation loss: 2630.5130678253627\n",
      "epoch : 51 / Train loss : 2627.942541228784 / Validation loss: 2612.237546625498\n",
      "epoch : 52 / Train loss : 2613.064771489601 / Validation loss: 2597.277035666392\n",
      "epoch : 53 / Train loss : 2602.8413320830755 / Validation loss: 2591.7707267426263\n",
      "epoch : 54 / Train loss : 2602.1591035138495 / Validation loss: 2591.8217531304117\n",
      "epoch : 55 / Train loss : 2605.442476816558 / Validation loss: 2588.759355366968\n",
      "epoch : 56 / Train loss : 2602.7952282113934 / Validation loss: 2581.4620857180917\n",
      "epoch : 57 / Train loss : 2593.1367106267267 / Validation loss: 2576.581650171405\n",
      "epoch : 58 / Train loss : 2584.348563951852 / Validation loss: 2577.6176985736265\n",
      "epoch : 59 / Train loss : 2581.5154463996532 / Validation loss: 2580.647496269105\n",
      "epoch : 60 / Train loss : 2581.875771604823 / Validation loss: 2579.721108957323\n",
      "epoch : 61 / Train loss : 2579.998062014776 / Validation loss: 2572.737063906842\n",
      "epoch : 62 / Train loss : 2573.806325270027 / Validation loss: 2562.6482396146375\n",
      "epoch : 63 / Train loss : 2565.9105596259587 / Validation loss: 2554.320849071236\n",
      "epoch : 64 / Train loss : 2560.454354211377 / Validation loss: 2549.7741076416946\n",
      "epoch : 65 / Train loss : 2558.4084896669647 / Validation loss: 2546.651134333087\n",
      "epoch : 66 / Train loss : 2556.4021592855847 / Validation loss: 2542.3679906732623\n",
      "epoch : 67 / Train loss : 2551.4170572448556 / Validation loss: 2537.9827619587963\n",
      "epoch : 68 / Train loss : 2544.906579817813 / Validation loss: 2535.8436465996874\n",
      "epoch : 69 / Train loss : 2540.160034328546 / Validation loss: 2535.5586169520907\n",
      "epoch : 70 / Train loss : 2537.7342256430243 / Validation loss: 2534.3706319321172\n",
      "epoch : 71 / Train loss : 2535.4666237203755 / Validation loss: 2530.408069857508\n",
      "epoch : 72 / Train loss : 2531.6566710357865 / Validation loss: 2524.4869775857433\n",
      "epoch : 73 / Train loss : 2526.924217304508 / Validation loss: 2518.9534533214382\n",
      "epoch : 74 / Train loss : 2523.1114719726515 / Validation loss: 2515.103874594447\n",
      "epoch : 75 / Train loss : 2520.816236856626 / Validation loss: 2512.1765463438273\n",
      "epoch : 76 / Train loss : 2518.636337385769 / Validation loss: 2509.12823506492\n",
      "epoch : 77 / Train loss : 2515.2634056893526 / Validation loss: 2506.321806951374\n",
      "epoch : 78 / Train loss : 2511.3056365165908 / Validation loss: 2504.5014473942715\n",
      "epoch : 79 / Train loss : 2508.0940971183677 / Validation loss: 2503.11236263976\n",
      "epoch : 80 / Train loss : 2505.685834257759 / Validation loss: 2500.716397354966\n",
      "epoch : 81 / Train loss : 2503.033559503348 / Validation loss: 2496.6692812625383\n",
      "epoch : 82 / Train loss : 2499.5766641573528 / Validation loss: 2491.7720602013337\n",
      "epoch : 83 / Train loss : 2495.9005388837113 / Validation loss: 2487.2587320180423\n",
      "epoch : 84 / Train loss : 2492.7926107079184 / Validation loss: 2483.5136399867024\n",
      "epoch : 85 / Train loss : 2490.117567505599 / Validation loss: 2480.165216270884\n",
      "epoch : 86 / Train loss : 2487.1441454005035 / Validation loss: 2477.1224232968384\n",
      "epoch : 87 / Train loss : 2483.767803157131 / Validation loss: 2474.688667287261\n",
      "epoch : 88 / Train loss : 2480.615447827414 / Validation loss: 2472.694077317289\n",
      "epoch : 89 / Train loss : 2477.9821427928005 / Validation loss: 2470.3478500000765\n",
      "epoch : 90 / Train loss : 2475.4496561231053 / Validation loss: 2467.1452125888336\n",
      "epoch : 91 / Train loss : 2472.645142352618 / Validation loss: 2463.4673734393155\n",
      "epoch : 92 / Train loss : 2469.8054579257855 / Validation loss: 2460.0740842503096\n",
      "epoch : 93 / Train loss : 2467.331959830294 / Validation loss: 2457.287223748986\n",
      "epoch : 94 / Train loss : 2465.133464946675 / Validation loss: 2454.9931771799284\n",
      "epoch : 95 / Train loss : 2462.8391137059684 / Validation loss: 2453.165913671556\n",
      "epoch : 96 / Train loss : 2460.462659745114 / Validation loss: 2451.827481696051\n",
      "epoch : 97 / Train loss : 2458.3264632672367 / Validation loss: 2450.641140599741\n",
      "epoch : 98 / Train loss : 2456.4682981874607 / Validation loss: 2449.1033052935927\n",
      "epoch : 99 / Train loss : 2454.635003417005 / Validation loss: 2447.146705859704\n",
      "epoch : 100 / Train loss : 2452.7739398485137 / Validation loss: 2445.2059217988167\n",
      "epoch : 101 / Train loss : 2451.1094426810077 / Validation loss: 2443.68615006101\n",
      "epoch : 102 / Train loss : 2449.7256989303924 / Validation loss: 2442.6614583277806\n",
      "epoch : 103 / Train loss : 2448.4450167402165 / Validation loss: 2442.090600284928\n",
      "epoch : 104 / Train loss : 2447.1928816503205 / Validation loss: 2441.9256745445796\n",
      "epoch : 105 / Train loss : 2446.1194778669337 / Validation loss: 2441.921988925936\n",
      "epoch : 106 / Train loss : 2445.279227409418 / Validation loss: 2441.719373720084\n",
      "epoch : 107 / Train loss : 2444.5345160173133 / Validation loss: 2441.2266383930846\n",
      "epoch : 108 / Train loss : 2443.8285537246675 / Validation loss: 2440.705533242386\n",
      "epoch : 109 / Train loss : 2443.2648239599407 / Validation loss: 2440.4351046483494\n",
      "epoch : 110 / Train loss : 2442.8714661234226 / Validation loss: 2440.49574881826\n",
      "epoch : 111 / Train loss : 2442.544574823559 / Validation loss: 2440.8629621508867\n",
      "epoch : 112 / Train loss : 2442.257664539104 / Validation loss: 2441.433800044556\n",
      "epoch : 113 / Train loss : 2442.085584085865 / Validation loss: 2441.966625488563\n",
      "epoch : 114 / Train loss : 2442.0192464434017 / Validation loss: 2442.2368844974885\n",
      "epoch : 115 / Train loss : 2441.9739965855492 / Validation loss: 2442.2747593176327\n",
      "epoch : 116 / Train loss : 2441.9483000260266 / Validation loss: 2442.2969721145705\n",
      "epoch : 117 / Train loss : 2441.9875102055703 / Validation loss: 2442.4626916290863\n",
      "epoch : 118 / Train loss : 2442.0575341297754 / Validation loss: 2442.800237432443\n",
      "epoch : 119 / Train loss : 2442.102884810548 / Validation loss: 2443.2513174047403\n",
      "epoch : 120 / Train loss : 2442.143935152062 / Validation loss: 2443.6642568077964\n",
      "epoch : 121 / Train loss : 2442.2034108566795 / Validation loss: 2443.860982134622\n",
      "epoch : 122 / Train loss : 2442.246302075202 / Validation loss: 2443.804411158962\n",
      "epoch : 123 / Train loss : 2442.255719616601 / Validation loss: 2443.631621173699\n",
      "epoch : 124 / Train loss : 2442.261554379465 / Validation loss: 2443.5009719662485\n",
      "epoch : 125 / Train loss : 2442.267593856169 / Validation loss: 2443.4812256287137\n",
      "epoch : 126 / Train loss : 2442.2487588286335 / Validation loss: 2443.5535598795454\n",
      "epoch : 127 / Train loss : 2442.2107812389986 / Validation loss: 2443.6244596909733\n",
      "epoch : 128 / Train loss : 2442.174748047322 / Validation loss: 2443.571873303505\n",
      "epoch : 129 / Train loss : 2442.1310366153575 / Validation loss: 2443.3508753349365\n",
      "epoch : 130 / Train loss : 2442.0688974719774 / Validation loss: 2443.038067652651\n",
      "epoch : 131 / Train loss : 2442.002457001221 / Validation loss: 2442.7532622023045\n",
      "epoch : 132 / Train loss : 2441.9401098307058 / Validation loss: 2442.5666828154353\n",
      "epoch : 133 / Train loss : 2441.872232529786 / Validation loss: 2442.477226096489\n",
      "epoch : 134 / Train loss : 2441.7994389384235 / Validation loss: 2442.424000864715\n",
      "epoch : 135 / Train loss : 2441.7340149983575 / Validation loss: 2442.3194917946344\n",
      "epoch : 136 / Train loss : 2441.6748350261546 / Validation loss: 2442.121311483113\n",
      "epoch : 137 / Train loss : 2441.614936880916 / Validation loss: 2441.8695706364006\n",
      "epoch : 138 / Train loss : 2441.5598497681763 / Validation loss: 2441.6448349422158\n",
      "epoch : 139 / Train loss : 2441.514386605166 / Validation loss: 2441.5023039104426\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "#model = TimeSeriesLSTMWithAttention(input_size, hidden_size, num_layers, 2)\n",
    "model = TimeSeriesDNN(input_size, hidden_size, num_layers, 2)\n",
    "#model = TimeSeriesCNN(input_size, hidden_size, num_layers)\n",
    "\n",
    "# # 모델 파라미터 정의\n",
    "# input_size = X_train.shape[2]  # 입력 데이터의 특성 수\n",
    "# d_model = 64  # Transformer 레이어의 출력 차원\n",
    "# nhead = 4  # Multi-Head Self-Attention의 헤드 수\n",
    "# num_filters = 32  # Conv1d의 출력 채널 수\n",
    "# kernel_size = 3  # Conv1d의 커널 크기\n",
    "# model = TimeSeriesTransformerWithAttentionAndConv1d(d_model=64, nhead=4, num_filters=32, kernel_size=3)\n",
    "\n",
    "# 모델 훈련 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "#criterion = torch.nn.MSELoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Validation 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Early stopping 관련 설정\n",
    "best_val_loss = float('inf')\n",
    "patience = 30  # 일정 횟수 동안 검증 손실이 향상되지 않을 때 조기 종료\n",
    "counter = 0\n",
    "epoch = 0\n",
    "\n",
    "while 1:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation 손실 확인\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    epoch += 1\n",
    "    print(f\"epoch : {epoch} / Train loss : {math.sqrt(criterion(outputs, y_train_tensor))} / Validation loss: {math.sqrt(criterion(val_outputs, y_val_tensor))}\")\n",
    "        # 검증 손실이 이전 최고 손실보다 낮으면 모델 가중치 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './weights/best_model.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    # 검증 손실이 일정 횟수 동안 향상되지 않으면 조기 종료\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 제곱 오차 (RMSE): 1729.6001705596586\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터로 평가\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    mse = mean_squared_error(y_test_tensor, test_outputs.numpy())\n",
    "\n",
    "print(\"평균 제곱 오차 (RMSE):\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Transformer\n",
    "# # 모델 생성 및 훈련\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "# num_heads = 8\n",
    "# model = TimeSeriesTransformer(input_size, hidden_size, num_layers, num_heads)\n",
    "\n",
    "# # 모델 훈련 설정\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "# # Validation 데이터 분할\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# # Early stopping 관련 설정\n",
    "# best_val_loss = float('inf')\n",
    "# patience = 10  # 일정 횟수 동안 검증 손실이 향상되지 않을 때 조기 종료\n",
    "# counter = 0\n",
    "\n",
    "# for epoch in range(1000):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train_tensor)\n",
    "#     loss = criterion(outputs, y_train_tensor)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Validation 손실 확인\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_outputs = model(X_val_tensor)\n",
    "#         val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    \n",
    "#     print(f\"epoch : {epoch} / Train loss : {math.sqrt(loss)} / Validation loss: {math.sqrt(val_loss)}\")\n",
    "\n",
    "#         # 검증 손실이 이전 최고 손실보다 낮으면 모델 가중치 저장\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), './weights/best_model.pth')\n",
    "#         counter = 0\n",
    "#     else:\n",
    "#         counter += 1\n",
    "    \n",
    "#     # 검증 손실이 일정 횟수 동안 향상되지 않으면 조기 종료\n",
    "#     if counter >= patience:\n",
    "#         print(\"Early stopping.\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
