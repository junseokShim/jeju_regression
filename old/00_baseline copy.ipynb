{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH  = './data/'\n",
    "MODEL_PATH = './models/'\n",
    "SUBMISSION_PATH = './submission/'\n",
    "\n",
    "TRAIN_SET = 'train.csv'\n",
    "TEST_SET  = 'test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Random-Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(DATA_PATH + TRAIN_SET)\n",
    "# submit_df = pd.read_csv(DATA_PATH + TEST_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #시계열 특성을 학습에 반영하기 위해 timestamp를 월, 일, 시간으로 나눕니다\n",
    "# train_df['year'] = train_df['timestamp'].apply(lambda x : int(x[0:4]))\n",
    "# train_df['month'] = train_df['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "# train_df['day'] = train_df['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "\n",
    "# submit_df['year'] = submit_df['timestamp'].apply(lambda x : int(x[0:4]))\n",
    "# submit_df['month'] = submit_df['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "# submit_df['day'] = submit_df['timestamp'].apply(lambda x : int(x[8:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #학습에 사용하지 않을 변수들을 제거합니다\n",
    "# train_df = train_df.drop(columns=['ID', 'supply(kg)'])\n",
    "# submit_df = submit_df.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #질적 변수들을 수치화합니다\n",
    "# qual_col = ['item', 'corporation', 'location']\n",
    "\n",
    "# for i in qual_col:\n",
    "#     le = LabelEncoder()\n",
    "#     train_df[i]=le.fit_transform(train_df[i])\n",
    "#     submit_df[i]=le.transform(submit_df[i]) #test 데이터에 대해서 fit하는 것은 data leakage에 해당합니다\n",
    "\n",
    "# print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df, _, _ = train_test_split(train_df, train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTSF_Linear(torch.nn.Module):\n",
    "    def __init__(self, window_size, forcast_size, individual, feature_size):\n",
    "        super(LTSF_Linear, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.forcast_size = forcast_size\n",
    "        self.individual = individual\n",
    "        self.channels = feature_size\n",
    "        if self.individual:\n",
    "            self.Linear = torch.nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "        else:\n",
    "            self.Linear = torch.nn.Linear(self.window_size, self.forcast_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.individual:\n",
    "            output = torch.zeros([x.size(0), self.forcast_size, x.size(2)],dtype=x.dtype).to(x.device)\n",
    "            for i in range(self.channels):\n",
    "                output[:,:,i] = self.Linear[i](x[:,:,i])\n",
    "            x = output\n",
    "        else:\n",
    "            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(torch.nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        residual = x - moving_mean\n",
    "        return moving_mean, residual \n",
    "        \n",
    "\n",
    "class LTSF_DLinear(torch.nn.Module):\n",
    "    def __init__(self, window_size, forcast_size, kernel_size, individual, feature_size):\n",
    "        super(LTSF_DLinear, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.forcast_size = forcast_size\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.individual = individual\n",
    "        self.channels = feature_size\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = torch.nn.ModuleList()\n",
    "            self.Linear_Trend = torch.nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Trend.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "                self.Linear_Trend[i].weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "                self.Linear_Seasonal.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "                self.Linear_Seasonal[i].weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "        else:\n",
    "            self.Linear_Trend = torch.nn.Linear(self.window_size, self.forcast_size)\n",
    "            self.Linear_Trend.weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "            self.Linear_Seasonal = torch.nn.Linear(self.window_size,  self.forcast_size)\n",
    "            self.Linear_Seasonal.weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend_init, seasonal_init = self.decompsition(x)\n",
    "        trend_init, seasonal_init = trend_init.permute(0,2,1), seasonal_init.permute(0,2,1)\n",
    "        if self.individual:\n",
    "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.forcast_size], dtype=trend_init.dtype).to(trend_init.device)\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.forcast_size], dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            for idx in range(self.channels):\n",
    "                trend_output[:, idx, :] = self.Linear_Trend[idx](trend_init[:, idx, :])\n",
    "                seasonal_output[:, idx, :] = self.Linear_Seasonal[idx](seasonal_init[:, idx, :])                \n",
    "        else:\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        x = seasonal_output + trend_output\n",
    "        return x.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(train_df, test_df, not_col, target):\n",
    "    train_df_ = train_df.copy()\n",
    "    test_df_ = test_df.copy()\n",
    "    submit_df_ = submit_df.copy()\n",
    "    col =  [col for col in list(train_df.columns) if col not in [not_col]]\n",
    "    mean_list = []\n",
    "    std_list = []\n",
    "    for x in col:\n",
    "        mean, std = train_df_.agg([\"mean\", \"std\"]).loc[:,x]\n",
    "        mean_list.append(mean)\n",
    "        std_list.append(std)\n",
    "        train_df_.loc[:, x] = (train_df_[x] - mean) / std\n",
    "        test_df_.loc[:, x] = (test_df_[x] - mean) / std\n",
    "    return train_df_, test_df_, mean_list[col.index(target)], std_list[col.index(target)]\n",
    "\n",
    "def min_max_scaling(train_df, test_df, not_col, target):\n",
    "    train_df_ = train_df.copy()\n",
    "    test_df_ = test_df.copy()\n",
    "    \n",
    "    # Exclude 'not_col' from the list of columns\n",
    "    col = [col for col in list(train_df.columns) if col != not_col]\n",
    "    \n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    \n",
    "    for x in col:\n",
    "        min_val = train_df_[x].min()\n",
    "        max_val = train_df_[x].max()\n",
    "        \n",
    "        min_list.append(min_val)\n",
    "        max_list.append(max_val)\n",
    "        \n",
    "        # Apply Min-Max scaling\n",
    "        train_df_[x] = (train_df_[x] - min_val) / (max_val - min_val)\n",
    "        test_df_[x] = (test_df_[x] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return train_df_, test_df_, min_list, max_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def time_slide_df(df, window_size, forcast_size, date, target):\n",
    "    df_ = df.copy()\n",
    "    df_ = df_.reset_index()\n",
    "    data_list = []\n",
    "    dap_list = []\n",
    "    date_list = []\n",
    "    for idx in range(0, df_.shape[0]-window_size-forcast_size+1):\n",
    "        x = df_.loc[idx:idx+window_size-1, target].values.reshape(window_size, 1)\n",
    "        y = df_.loc[idx+window_size:idx+window_size+forcast_size-1, target].values\n",
    "        date_ = df_.loc[idx+window_size:idx+window_size+forcast_size-1, date].values\n",
    "        data_list.append(x)\n",
    "        dap_list.append(y)\n",
    "        date_list.append(date_)\n",
    "    return np.array(data_list, dtype='float32'), np.array(dap_list, dtype='float32'), np.array(date_list)\n",
    "\n",
    "\n",
    "def time_slide_result_df(df, window_size, forcast_size, date, target):\n",
    "    df_ = df.copy()\n",
    "    df_ = df_.reset_index()\n",
    "    data_list = []\n",
    "    dap_list = []\n",
    "    date_list = []\n",
    "    x = df_.loc[-window_size:, target].values.reshape(window_size, 1)\n",
    "    return np.array(x, dtype='float32')\n",
    "    \n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Univariable ###\n",
    "# ### 데이터 셋 생성 ###\n",
    "# window_size = 56\n",
    "# forcast_size= 28\n",
    "# batch_size = 32\n",
    "# targets = 'price(원/kg)'\n",
    "# date = 'timestamp'\n",
    "\n",
    "# train_df_fe, test_df_fe, submit_fe, mean_, std_ = standardization(train_df, test_df, submit_df, 'timestamp', targets)\n",
    "# train_x, train_y, train_date = time_slide_df(train_df_fe, window_size, forcast_size, date, targets)\n",
    "# test_x, test_y, test_date = time_slide_df(test_df_fe, window_size, forcast_size, date, targets)\n",
    "\n",
    "# train_ds = Data(train_x[:1000], train_y[:1000])\n",
    "# valid_ds = Data(train_x[1000:], train_y[1000:])\n",
    "# test_ds = Data(test_x, test_y)\n",
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True,)\n",
    "# valid_dl = DataLoader(valid_ds, batch_size = train_x[1000:].shape[0], shuffle=False)\n",
    "# test_dl  = DataLoader(test_ds,  batch_size = test_x.shape[0], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 모델 학습 ###\n",
    "# train_loss_list = []\n",
    "# valid_loss_list = []\n",
    "# test_loss_list = []\n",
    "# epoch = 50\n",
    "# lr = 0.001\n",
    "# DLinear_model = LTSF_DLinear(\n",
    "#                             window_size=window_size,\n",
    "#                             forcast_size=forcast_size,\n",
    "#                             kernel_size=25,\n",
    "#                             individual=False,\n",
    "#                             feature_size=1,\n",
    "#                             )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(DLinear_model.parameters(), lr=lr)\n",
    "# max_loss = 999999999\n",
    "\n",
    "# for epoch in tqdm(range(1, epoch+1)):\n",
    "#     loss_list = []\n",
    "#     DLinear_model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_dl):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = DLinear_model(data)\n",
    "#         loss = criterion(output, target.unsqueeze(-1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         loss_list.append(loss.item())    \n",
    "#     train_loss_list.append(np.mean(loss_list))\n",
    "\n",
    "#     DLinear_model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in valid_dl:\n",
    "#             output = DLinear_model(data)\n",
    "#             valid_loss = criterion(output, target.unsqueeze(-1))\n",
    "#             valid_loss_list.append(valid_loss)\n",
    "        \n",
    "#         for data, target in test_dl:\n",
    "#             output = DLinear_model(data)\n",
    "#             test_loss = criterion(output, target.unsqueeze(-1))\n",
    "#             test_loss_list.append(test_loss)\n",
    "\n",
    "#     if valid_loss < max_loss:\n",
    "#         torch.save(DLinear_model, 'DLinear_model.pth')\n",
    "#         max_loss = valid_loss\n",
    "#         print(\"valid_loss={:.3f}, test_los{:.3f}, Model Save\".format(valid_loss, test_loss))\n",
    "#         dlinear_best_epoch = epoch\n",
    "#         dlinear_best_train_loss = np.mean(loss_list)\n",
    "#         dlinear_best_valid_loss = np.mean(valid_loss.item())\n",
    "#         dlinear_best_test_loss = np.mean(test_loss.item())\n",
    "\n",
    "#     print(\"epoch = {}, train_loss : {:.3f}, valid_loss : {:.3f}, test_loss : {:.3f}\".format(epoch, np.mean(loss_list), valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Validations\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# weights_list = {}\n",
    "# weights_list['trend'] = DLinear_model.Linear_Trend.weight.detach().numpy()\n",
    "# weights_list['seasonal'] = DLinear_model.Linear_Seasonal.weight.detach().numpy()\n",
    "\n",
    "# for name, w in weights_list.items():    \n",
    "#     fig, ax = plt.subplots()    \n",
    "#     plt.title(name)\n",
    "#     im = ax.imshow(w, cmap='plasma_r',)\n",
    "#     fig.colorbar(im, pad=0.03)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_standardization(data, mean, std):\n",
    "    \"\"\"\n",
    "    정규화된 데이터를 원래 값으로 되돌립니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 정규화된 데이터\n",
    "    - mean: 평균\n",
    "    - std: 표준편차\n",
    "    \n",
    "    Returns:\n",
    "    - 원래 값으로 복구된 데이터\n",
    "    \"\"\"\n",
    "    return data * std + mean\n",
    "\n",
    "def convert_standardization(data, mean, std):\n",
    "    \"\"\"\n",
    "    지정된 평균 및 표준편차 값으로 정규화된 데이터를 진행합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 정규화된 데이터\n",
    "    - mean: 평균\n",
    "    - std: 표준편차\n",
    "    \n",
    "    Returns:\n",
    "    - 지정된 평균 및 표준편차 값으로 정규화된 데이터\n",
    "    \"\"\"\n",
    "    return (data-mean)/std\n",
    "\n",
    "\n",
    "def reverse_min_max_scaling(data, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Min-Max 스케일링된 데이터를 원래 값으로 되돌립니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Min-Max 스케일링된 데이터\n",
    "    - min_val: 해당 feature의 최솟값\n",
    "    - max_val: 해당 feature의 최댓값\n",
    "    \n",
    "    Returns:\n",
    "    - 원래 값으로 복구된 데이터\n",
    "    \"\"\"\n",
    "    return data * (max_val - min_val) + min_val\n",
    "\n",
    "\n",
    "def convert_min_max_scaling(data, min_val, max_val):\n",
    "    \"\"\"\n",
    "    지정된 최솟값과 최댓값을 사용하여 Min-Max 스케일링을 진행합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 원래 값의 데이터\n",
    "    - min_val: 해당 feature의 최솟값\n",
    "    - max_val: 해당 feature의 최댓값\n",
    "    \n",
    "    Returns:\n",
    "    - Min-Max 스케일링된 데이터\n",
    "    \"\"\"\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataframe = pd.read_csv(DATA_PATH + TRAIN_SET)\n",
    "# train_dataframe['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 출력 제한 해제\n",
    "# pd.set_option('display.max_columns',None)\n",
    "# pd.set_option('display.max_rows',None)\n",
    "# pd.set_option('display.max_seq_items', None)\n",
    "# pd.options.display.max_colwidth = 100\n",
    "\n",
    "# train_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataframe = pd.read_csv(DATA_PATH + TRAIN_SET)\n",
    "# results = []\n",
    "# lens = 0\n",
    "\n",
    "# for i in list(train_dataframe['item'].unique()):\n",
    "#     for k in train_dataframe[train_dataframe['item']==i]['corporation'].unique():\n",
    "#         for j in ['J','S']:\n",
    "#             print(f'item : {i} , corporation : {k} , location : {j} ')\n",
    "#             train_df = train_dataframe[(train_dataframe['item']==i)&\\\n",
    "#                     (train_dataframe['location']==j)&\\\n",
    "#                     (train_dataframe['corporation']==k)]\n",
    "#             output_result = train_df.copy()\n",
    "#             lens += len(train_df)\n",
    "#             if len(train_df) == 0: \n",
    "#                 print('gd')\n",
    "#                 continue\n",
    "\n",
    "#             train_df['year'] = train_df['timestamp'].apply(lambda x : int(x[0:4]))\n",
    "#             train_df['month'] = train_df['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "#             train_df['day'] = train_df['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "            \n",
    "#             #학습에 사용하지 않을 변수들을 제거합니다\n",
    "#             train_df = train_df.drop(columns=['ID', 'supply(kg)'])\n",
    "#             #submit_df = submit_df.drop(columns=['ID'])\n",
    "\n",
    "#             #질적 변수들을 수치화합니다\n",
    "#             qual_col = ['item', 'corporation', 'location']\n",
    "\n",
    "#             for idx in qual_col:\n",
    "#                 le = LabelEncoder()\n",
    "#                 train_df[idx]=le.fit_transform(train_df[idx])\n",
    "#                 #submit_df[i]=le.transform(submit_df[i]) #test 데이터에 대해서 fit하는 것은 data leakage에 해당합니다\n",
    "\n",
    "#             train_df, test_df, _, _ = train_test_split(train_df, train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "#             ### 데이터 셋 생성 ###\n",
    "#             window_size = 56\n",
    "#             forcast_size= 28\n",
    "#             batch_size = 32\n",
    "#             targets = 'price(원/kg)'\n",
    "#             date = 'timestamp'\n",
    "\n",
    "#             train_df_fe, test_df_fe, mean_, std_ = standardization(train_df, test_df, 'timestamp', targets)\n",
    "#             train_x, train_y, train_date = time_slide_df(train_df_fe, window_size, forcast_size, date, targets)\n",
    "#             test_x, test_y, test_date = time_slide_df(test_df_fe, window_size, forcast_size, date, targets)\n",
    "\n",
    "#             train_ds = Data(train_x[:1000], train_y[:1000])\n",
    "#             valid_ds = Data(train_x[1000:], train_y[1000:])\n",
    "#             test_ds = Data(test_x, test_y)\n",
    "\n",
    "#             train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True,)\n",
    "#             valid_dl = DataLoader(valid_ds, batch_size = train_x[1000:].shape[0], shuffle=False)\n",
    "#             test_dl  = DataLoader(test_ds,  batch_size = test_x.shape[0], shuffle=False)\n",
    "\n",
    "#             ### 모델 학습 ###\n",
    "#             train_loss_list = []\n",
    "#             valid_loss_list = []\n",
    "#             test_loss_list = []\n",
    "#             epoch = 50\n",
    "#             lr = 0.001\n",
    "#             DLinear_model = LTSF_DLinear(\n",
    "#                                         window_size=window_size,\n",
    "#                                         forcast_size=forcast_size,\n",
    "#                                         kernel_size=25,\n",
    "#                                         individual=False,\n",
    "#                                         feature_size=1,\n",
    "#                                         )\n",
    "#             criterion = torch.nn.MSELoss()\n",
    "#             optimizer = torch.optim.Adam(DLinear_model.parameters(), lr=lr)\n",
    "#             max_loss = 999999999\n",
    "\n",
    "#             for epoch in tqdm(range(1, epoch+1)):\n",
    "#                 loss_list = []\n",
    "#                 DLinear_model.train()\n",
    "#                 for batch_idx, (data, target) in enumerate(train_dl):\n",
    "#                     optimizer.zero_grad()\n",
    "#                     output = DLinear_model(data)\n",
    "#                     loss = criterion(output, target.unsqueeze(-1))\n",
    "#                     loss.backward()\n",
    "#                     optimizer.step()\n",
    "#                     loss_list.append(loss.item())    \n",
    "#                 train_loss_list.append(np.mean(loss_list))\n",
    "\n",
    "#                 DLinear_model.eval()\n",
    "#                 with torch.no_grad():\n",
    "#                     for data, target in valid_dl:\n",
    "#                         output = DLinear_model(data)\n",
    "#                         valid_loss = criterion(output, target.unsqueeze(-1))\n",
    "#                         valid_loss_list.append(valid_loss)\n",
    "                    \n",
    "#                     for data, target in test_dl:\n",
    "#                         output = DLinear_model(data)\n",
    "#                         test_loss = criterion(output, target.unsqueeze(-1))\n",
    "#                         test_loss_list.append(test_loss)\n",
    "\n",
    "#                 if valid_loss < max_loss:\n",
    "#                     torch.save(DLinear_model, 'DLinear_model.pth')\n",
    "#                     max_loss = valid_loss\n",
    "#                     #print(\"valid_loss={:.3f}, test_los{:.3f}, Model Save\".format(valid_loss, test_loss))\n",
    "#                     dlinear_best_epoch = epoch\n",
    "#                     dlinear_best_train_loss = np.mean(loss_list)\n",
    "#                     dlinear_best_valid_loss = np.mean(valid_loss.item())\n",
    "#                     dlinear_best_test_loss = np.mean(test_loss.item())\n",
    "\n",
    "#                 #print(\"epoch = {}, train_loss : {:.3f}, valid_loss : {:.3f}, test_loss : {:.3f}\".format(epoch, np.mean(loss_list), valid_loss, test_loss))\n",
    "\n",
    "#             output_result['year'] = output_result['timestamp'].apply(lambda x : int(x[0:4]))\n",
    "#             output_result['month'] = output_result['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "#             output_result['day'] = output_result['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "#             output_result = output_result.drop(columns=['ID', 'supply(kg)'])\n",
    "\n",
    "#             qual_col = ['item', 'corporation', 'location']\n",
    "#             for idx in qual_col:\n",
    "#                 le = LabelEncoder()\n",
    "#                 output_result[idx]=le.fit_transform(output_result[idx])\n",
    "\n",
    "#             last_56_days_data = np.array(output_result[-window_size:]['price(원/kg)']).reshape(1, window_size, 1)\n",
    "#             last_56_days_data = convert_standardization(last_56_days_data, mean_, std_)\n",
    "#             last_56_days_data = torch.tensor(last_56_days_data, dtype=torch.float32)\n",
    "\n",
    "#             DLinear_model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 predictions = DLinear_model(last_56_days_data)\n",
    "\n",
    "#             predictions = reverse_standardization(predictions, mean_, std_)\n",
    "#             predictions = predictions.reshape(28)\n",
    "#             results.extend(np.array(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : A , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : A , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : B , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : B , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : C , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : C , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : D , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : D , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : E , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : TG , corporation : E , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CR , corporation : A , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CR , corporation : A , location : S \n",
      "gd\n",
      "item : CR , corporation : B , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 47.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CR , corporation : B , location : S \n",
      "gd\n",
      "item : CR , corporation : C , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CR , corporation : C , location : S \n",
      "gd\n",
      "item : CR , corporation : D , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CR , corporation : D , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CR , corporation : E , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CR , corporation : E , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CB , corporation : A , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CB , corporation : A , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CB , corporation : D , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CB , corporation : D , location : S \n",
      "gd\n",
      "item : CB , corporation : E , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CB , corporation : E , location : S \n",
      "gd\n",
      "item : CB , corporation : F , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : CB , corporation : F , location : S \n",
      "gd\n",
      "item : RD , corporation : A , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : RD , corporation : A , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : RD , corporation : C , location : J \n",
      "gd\n",
      "item : RD , corporation : C , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : RD , corporation : D , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : RD , corporation : D , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : RD , corporation : E , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : RD , corporation : E , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : RD , corporation : F , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : RD , corporation : F , location : S \n",
      "gd\n",
      "item : BC , corporation : A , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : BC , corporation : A , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : BC , corporation : B , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : BC , corporation : B , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : BC , corporation : C , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : BC , corporation : C , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : BC , corporation : D , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : BC , corporation : D , location : S \n",
      "gd\n",
      "item : BC , corporation : E , location : J \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item : BC , corporation : E , location : S \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.17it/s]\n"
     ]
    }
   ],
   "source": [
    "def min_max_standardization(train_df, test_df, not_col, target):\n",
    "    train_df_ = train_df.copy()\n",
    "    test_df_ = test_df.copy()\n",
    "    col = [col for col in list(train_df.columns) if col not in [not_col]]\n",
    "    \n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    \n",
    "    for x in col:\n",
    "        min_val = train_df_[x].min()\n",
    "        max_val = train_df_[x].max()\n",
    "        min_list.append(min_val)\n",
    "        max_list.append(max_val)\n",
    "        \n",
    "        train_df_[x] = (train_df_[x] - min_val) / (max_val - min_val)\n",
    "        test_df_[x] = (test_df_[x] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return train_df_, test_df_, min_list[col.index(target)], max_list[col.index(target)]\n",
    "\n",
    "def min_max_scaling(data, min_val, max_val):\n",
    "    \"\"\"\n",
    "    주어진 데이터를 Min-Max 스케일링합니다.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 스케일링할 데이터\n",
    "    - min_val: 데이터에서의 최솟값\n",
    "    - max_val: 데이터에서의 최댓값\n",
    "\n",
    "    Returns:\n",
    "    - Min-Max 스케일링된 데이터\n",
    "    \"\"\"\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "def min_max_reverse_scaling(data, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Min-Max 스케일링된 데이터를 원래 값으로 복구합니다.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Min-Max 스케일링된 데이터\n",
    "    - min_val: 데이터에서의 최솟값\n",
    "    - max_val: 데이터에서의 최댓값\n",
    "\n",
    "    Returns:\n",
    "    - 원래 값으로 복구된 데이터\n",
    "    \"\"\"\n",
    "    return data * (max_val - min_val) + min_val\n",
    "\n",
    "\n",
    "train_dataframe = pd.read_csv(DATA_PATH + TRAIN_SET)\n",
    "results = []\n",
    "lens = 0\n",
    "\n",
    "for i in list(train_dataframe['item'].unique()):\n",
    "    for k in train_dataframe[train_dataframe['item'] == i]['corporation'].unique():\n",
    "        for j in ['J', 'S']:\n",
    "            print(f'item : {i} , corporation : {k} , location : {j} ')\n",
    "            train_df = train_dataframe[(train_dataframe['item'] == i) & \\\n",
    "                    (train_dataframe['location'] == j) & \\\n",
    "                    (train_dataframe['corporation'] == k)]\n",
    "            output_result = train_df.copy()\n",
    "            lens += len(train_df)\n",
    "            if len(train_df) == 0:\n",
    "                print('gd')\n",
    "                continue\n",
    "\n",
    "            train_df['year'] = train_df['timestamp'].apply(lambda x: int(x[0:4]))\n",
    "            train_df['month'] = train_df['timestamp'].apply(lambda x: int(x[5:7]))\n",
    "            train_df['day'] = train_df['timestamp'].apply(lambda x: int(x[8:10]))\n",
    "\n",
    "            # 학습에 사용하지 않을 변수들을 제거합니다\n",
    "            train_df = train_df.drop(columns=['ID', 'supply(kg)'])\n",
    "            # submit_df = submit_df.drop(columns=['ID'])\n",
    "\n",
    "            # 질적 변수들을 수치화합니다\n",
    "            qual_col = ['item', 'corporation', 'location']\n",
    "\n",
    "            for idx in qual_col:\n",
    "                le = LabelEncoder()\n",
    "                train_df[idx] = le.fit_transform(train_df[idx])\n",
    "                # submit_df[i] = le.transform(submit_df[i]) # test 데이터에 대해서 fit하는 것은 data leakage에 해당합니다\n",
    "\n",
    "            train_df, test_df, _, _ = train_test_split(train_df, train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "            ### 데이터 셋 생성 ###\n",
    "            window_size = 28*1\n",
    "            forcast_size = 28\n",
    "            batch_size = 32\n",
    "            targets = 'price(원/kg)'\n",
    "            date = 'timestamp'\n",
    "\n",
    "            train_df_fe, test_df_fe, min_, max_ = min_max_standardization(train_df, test_df, 'timestamp', targets)\n",
    "            train_x, train_y, train_date = time_slide_df(train_df_fe, window_size, forcast_size, date, targets)\n",
    "            test_x, test_y, test_date = time_slide_df(test_df_fe, window_size, forcast_size, date, targets)\n",
    "\n",
    "            train_ds = Data(train_x[:1000], train_y[:1000])\n",
    "            valid_ds = Data(train_x[1000:], train_y[1000:])\n",
    "            test_ds = Data(test_x, test_y)\n",
    "\n",
    "            train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "            valid_dl = DataLoader(valid_ds, batch_size=train_x[1000:].shape[0], shuffle=False)\n",
    "            test_dl = DataLoader(test_ds, batch_size=test_x.shape[0], shuffle=False)\n",
    "\n",
    "            ### 모델 학습 ###\n",
    "            train_loss_list = []\n",
    "            valid_loss_list = []\n",
    "            test_loss_list = []\n",
    "            epoch = 100\n",
    "            lr = 0.0005\n",
    "            DLinear_model = LTSF_DLinear(\n",
    "                window_size=window_size,\n",
    "                forcast_size=forcast_size,\n",
    "                kernel_size=25,\n",
    "                individual=False,\n",
    "                feature_size=1\n",
    "            )\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(DLinear_model.parameters(), lr=lr)\n",
    "            max_loss = 999999999\n",
    "\n",
    "            for epoch in tqdm(range(1, epoch + 1)):\n",
    "                loss_list = []\n",
    "                DLinear_model.train()\n",
    "                for batch_idx, (data, target) in enumerate(train_dl):\n",
    "                    optimizer.zero_grad()\n",
    "                    output = DLinear_model(data)\n",
    "                    loss = criterion(output, target.unsqueeze(-1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    loss_list.append(loss.item())\n",
    "                train_loss_list.append(np.mean(loss_list))\n",
    "\n",
    "                DLinear_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for data, target in valid_dl:\n",
    "                        output = DLinear_model(data)\n",
    "                        valid_loss = criterion(output, target.unsqueeze(-1))\n",
    "                        valid_loss_list.append(valid_loss)\n",
    "\n",
    "                    for data, target in test_dl:\n",
    "                        output = DLinear_model(data)\n",
    "                        test_loss = criterion(output, target.unsqueeze(-1))\n",
    "                        test_loss_list.append(test_loss)\n",
    "\n",
    "                if valid_loss < max_loss:\n",
    "                    torch.save(DLinear_model, 'DLinear_model.pth')\n",
    "                    max_loss = valid_loss\n",
    "                    # print(\"valid_loss={:.3f}, test_los{:.3f}, Model Save\".format(valid_loss, test_loss))\n",
    "                    dlinear_best_epoch = epoch\n",
    "                    dlinear_best_train_loss = np.mean(loss_list)\n",
    "                    dlinear_best_valid_loss = np.mean(valid_loss.item())\n",
    "                    dlinear_best_test_loss = np.mean(test_loss.item())\n",
    "\n",
    "                # print(\"epoch = {}, train_loss : {:.3f}, valid_loss : {:.3f}, test_loss : {:.3f}\".format(epoch, np.mean(loss_list), valid_loss, test_loss))\n",
    "            \n",
    "            DLinear_model = torch.load('DLinear_model.pth')\n",
    "            output_result['year'] = output_result['timestamp'].apply(lambda x: int(x[0:4]))\n",
    "            output_result['month'] = output_result['timestamp'].apply(lambda x: int(x[5:7]))\n",
    "            output_result['day'] = output_result['timestamp'].apply(lambda x: int(x[8:10]))\n",
    "            output_result = output_result.drop(columns=['ID', 'supply(kg)'])\n",
    "\n",
    "            qual_col = ['item', 'corporation', 'location']\n",
    "            for idx in qual_col:\n",
    "                le = LabelEncoder()\n",
    "                output_result[idx] = le.fit_transform(output_result[idx])\n",
    "\n",
    "            last_56_days_data = np.array(output_result[-window_size:]['price(원/kg)']).reshape(1, window_size, 1)\n",
    "            last_56_days_data = min_max_scaling(last_56_days_data, min_, max_)\n",
    "            last_56_days_data = torch.tensor(last_56_days_data, dtype=torch.float32)\n",
    "\n",
    "            DLinear_model.eval()\n",
    "            with torch.no_grad():\n",
    "                predictions = DLinear_model(last_56_days_data)\n",
    "\n",
    "            predictions = min_max_reverse_scaling(predictions, min_, max_)\n",
    "            predictions = predictions.reshape(28)\n",
    "            results.extend(np.array(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results, columns = ['gd'])\n",
    "\n",
    "for i in range(len(results)):\n",
    "    if (i-1) % 7 == 0: # 1, 8, 15, 22\n",
    "        results.iloc[i,-1] = 0\n",
    "\n",
    "\n",
    "submission = pd.read_csv(DATA_PATH+'./sample_submission.csv')\n",
    "submission['answer'] = results\n",
    "\n",
    "submission.to_csv(SUBMISSION_PATH+'./d_linear_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TG_A_J_20230304</td>\n",
       "      <td>2461.746826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TG_A_J_20230305</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TG_A_J_20230306</td>\n",
       "      <td>2372.402832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TG_A_J_20230307</td>\n",
       "      <td>2234.895264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TG_A_J_20230308</td>\n",
       "      <td>2584.550049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>RD_F_J_20230327</td>\n",
       "      <td>1467.568481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>RD_F_J_20230328</td>\n",
       "      <td>1285.734131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>RD_F_J_20230329</td>\n",
       "      <td>954.388123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>RD_F_J_20230330</td>\n",
       "      <td>948.465088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>RD_F_J_20230331</td>\n",
       "      <td>1112.625366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID       answer\n",
       "0     TG_A_J_20230304  2461.746826\n",
       "1     TG_A_J_20230305     0.000000\n",
       "2     TG_A_J_20230306  2372.402832\n",
       "3     TG_A_J_20230307  2234.895264\n",
       "4     TG_A_J_20230308  2584.550049\n",
       "...               ...          ...\n",
       "1087  RD_F_J_20230327  1467.568481\n",
       "1088  RD_F_J_20230328  1285.734131\n",
       "1089  RD_F_J_20230329   954.388123\n",
       "1090  RD_F_J_20230330   948.465088\n",
       "1091  RD_F_J_20230331  1112.625366\n",
       "\n",
       "[1092 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TG_A_J_20230304</td>\n",
       "      <td>2593.700195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TG_A_J_20230305</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TG_A_J_20230306</td>\n",
       "      <td>2301.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TG_A_J_20230307</td>\n",
       "      <td>2439.053467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TG_A_J_20230308</td>\n",
       "      <td>2645.454102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>RD_F_J_20230327</td>\n",
       "      <td>1324.171509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>RD_F_J_20230328</td>\n",
       "      <td>1017.770020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>RD_F_J_20230329</td>\n",
       "      <td>397.086548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>RD_F_J_20230330</td>\n",
       "      <td>691.312317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>RD_F_J_20230331</td>\n",
       "      <td>984.294189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID       answer\n",
       "0     TG_A_J_20230304  2593.700195\n",
       "1     TG_A_J_20230305     0.000000\n",
       "2     TG_A_J_20230306  2301.097656\n",
       "3     TG_A_J_20230307  2439.053467\n",
       "4     TG_A_J_20230308  2645.454102\n",
       "...               ...          ...\n",
       "1087  RD_F_J_20230327  1324.171509\n",
       "1088  RD_F_J_20230328  1017.770020\n",
       "1089  RD_F_J_20230329   397.086548\n",
       "1090  RD_F_J_20230330   691.312317\n",
       "1091  RD_F_J_20230331   984.294189\n",
       "\n",
       "[1092 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 56\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TG_A_J_20230304</td>\n",
       "      <td>1603.417358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TG_A_J_20230305</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TG_A_J_20230306</td>\n",
       "      <td>2084.053955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TG_A_J_20230307</td>\n",
       "      <td>1671.266235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TG_A_J_20230308</td>\n",
       "      <td>2183.236816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>RD_F_J_20230327</td>\n",
       "      <td>1087.108032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>RD_F_J_20230328</td>\n",
       "      <td>1199.210083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>RD_F_J_20230329</td>\n",
       "      <td>1169.479614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>RD_F_J_20230330</td>\n",
       "      <td>1140.815796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>RD_F_J_20230331</td>\n",
       "      <td>1138.506348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID       answer\n",
       "0     TG_A_J_20230304  1603.417358\n",
       "1     TG_A_J_20230305     0.000000\n",
       "2     TG_A_J_20230306  2084.053955\n",
       "3     TG_A_J_20230307  1671.266235\n",
       "4     TG_A_J_20230308  2183.236816\n",
       "...               ...          ...\n",
       "1087  RD_F_J_20230327  1087.108032\n",
       "1088  RD_F_J_20230328  1199.210083\n",
       "1089  RD_F_J_20230329  1169.479614\n",
       "1090  RD_F_J_20230330  1140.815796\n",
       "1091  RD_F_J_20230331  1138.506348\n",
       "\n",
       "[1092 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 140\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
