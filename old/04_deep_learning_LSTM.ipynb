{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 데이터 불러오기 (시계열 가격 데이터)\n",
    "data = pd.read_csv('./processed_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터에 Min-Max 스케일링 적용\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = data.drop(columns=['price(원/kg)']).values\n",
    "X = scaler.fit_transform(X)\n",
    "#X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "# 입력 텍스트와 레이블 생성\n",
    "Y = data['price(원/kg)'].values\n",
    "Y = Y.reshape(Y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, Train loss: 5408471.0\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 훈련 세트와 테스트 세트로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 정의 트랜스포머\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        \n",
    "        # Transformer 모델 불러오기\n",
    "        transformer_config = BertConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "        )\n",
    "        self.transformer = BertModel(transformer_config)\n",
    "        \n",
    "        # Fully Connected Layer 추가\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.transformer(x)\n",
    "        pooled_output = outputs.last_hidden_state.mean(1)  # 각 시퀀스의 평균을 사용\n",
    "        out = self.fc(pooled_output)\n",
    "        return out\n",
    "\n",
    "# 데이터를 PyTorch 텐서로 변환합니다.\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)  # 정수 데이터 유형으로 변환\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# 모델 생성 및 훈련\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "model = TimeSeriesTransformer(input_size, hidden_size, num_layers, num_heads)\n",
    "\n",
    "# 모델 훈련 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"epoch: {epoch+1}, Train loss: {loss.item()}\")\n",
    "\n",
    "# 테스트 데이터를 PyTorch 텐서로 변환하고 모델을 사용하여 예측 수행\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "\n",
    "# 예측 결과를 활용하여 원하는 작업을 수행하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 (LSTM)\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "\n",
    "# 모델 정의 트랜스포머\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        \n",
    "        # Transformer 모델 불러오기\n",
    "        transformer_config = BertConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "        )\n",
    "        self.transformer = BertModel(transformer_config)\n",
    "        \n",
    "        # Fully Connected Layer 추가\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.transformer(x)\n",
    "        pooled_output = outputs.last_hidden_state.mean(1)  # 각 시퀀스의 평균을 사용\n",
    "        out = self.fc(pooled_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화 : LSTM\n",
    "# input_size = X_train.shape[2]  # 차원 수정\n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "# model = LSTMModel(input_size, hidden_size, num_layers)\n",
    "\n",
    "# 모델 초기화 : Transformer\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "model = TimeSeriesTransformer(input_size, hidden_size, num_layers, num_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"epoch : {epoch} / loss : {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Validation 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation 손실 확인\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    \n",
    "    print(f\"epoch : {epoch} / Train loss : {loss} / Validation loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 평가\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    mse = mean_squared_error(y_test_tensor, test_outputs.numpy())\n",
    "\n",
    "print(\"평균 제곱 오차 (MSE):\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_config = BertConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "        )\n",
    "transformer = BertModel(transformer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
